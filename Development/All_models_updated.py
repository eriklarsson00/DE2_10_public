# -*- coding: utf-8 -*-
"""Copy of Final_models_3-2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kiOuIIyalLvgzL8NHjQjmrYt5R-XUiHB
"""

#!pip install ray[tune]

import os
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import r2_score
from ray import tune
from ray.tune.schedulers import ASHAScheduler
from ray.air import session
import joblib
from datetime import datetime

start_time = datetime.now()
# Load the dataset
file_path = 'combined.csv'
data = pd.read_csv(file_path)

# Drop columns and handle missing values
data = data.drop(columns=['language', 'license', 'topics'], axis=1)
data = data.dropna().reset_index(drop=True)

main_test = pd.read_csv("5datapoints.csv")
drop = [i for i in main_test['full_name']]
# Step 3: Drop rows where 'full_name' is in values_to_drop
data = data[~data['full_name'].isin(drop)]

numerical_data = data.select_dtypes(exclude=['object'])
# Define the target column
df = numerical_data
X = df.drop(columns=['stars'], axis=1)
y = df['stars']

# X.describe()

# print(X.isnull().sum())

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define training functions for Ray Tune
def train_lr(config):
    model = LinearRegression(fit_intercept=config["fit_intercept"])
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    r2 = r2_score(y_test, y_pred)
    session.report({"r2": r2})

def train_ridge(config):
    model = Ridge(alpha=config["alpha"])
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    r2 = r2_score(y_test, y_pred)
    session.report({"r2": r2})

def train_lasso(config):
    model = Lasso(alpha=config["alpha"])
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    r2 = r2_score(y_test, y_pred)
    session.report({"r2": r2})

def train_rf(config):
    model = RandomForestRegressor(
        n_estimators=config["n_estimators"],
        max_depth=config["max_depth"],
        min_samples_split=config["min_samples_split"],
        random_state=42
    )
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    r2 = r2_score(y_test, y_pred)
    session.report({"r2": r2})

def train_gb(config):
    model = GradientBoostingRegressor(
        n_estimators=config["n_estimators"],
        max_depth=config["max_depth"],
        learning_rate=config["learning_rate"],
        random_state=42
    )
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    r2 = r2_score(y_test, y_pred)
    session.report({"r2": r2})

# Define the search space for hyperparameters
config_lr = {
    "fit_intercept": tune.choice([True, False])
}

config_ridge = {
    "alpha": tune.loguniform(1e-3, 1e2)
}

config_lasso = {
    "alpha": tune.loguniform(1e-3, 1e2)
}

config_rf = {
    "n_estimators": tune.randint(10, 100),
    "max_depth": tune.randint(1, 20),
    "min_samples_split": tune.randint(2, 10)
}

config_gb = {
    "n_estimators": tune.randint(10, 100),
    "max_depth": tune.randint(1, 10),
    "learning_rate": tune.loguniform(1e-3, 1e-1)
}

# Use ASHAScheduler for early stopping
scheduler = ASHAScheduler(metric="r2", mode="max")

# Run hyperparameter tuning
analysis_lr = tune.run(
    train_lr,
    resources_per_trial={"cpu": 1},
    config=config_lr,
    num_samples=10,
    scheduler=scheduler
)

analysis_ridge = tune.run(
    train_ridge,
    resources_per_trial={"cpu": 1},
    config=config_ridge,
    num_samples=10,
    scheduler=scheduler
)

analysis_lasso = tune.run(
    train_lasso,
    resources_per_trial={"cpu": 1},
    config=config_lasso,
    num_samples=10,
    scheduler=scheduler
)

analysis_rf = tune.run(
    train_rf,
    resources_per_trial={"cpu": 1},
    config=config_rf,
    num_samples=10,
    scheduler=scheduler
)

analysis_gb = tune.run(
    train_gb,
    resources_per_trial={"cpu": 1},
    config=config_gb,
    num_samples=10,
    scheduler=scheduler
)

# Get the best result
best_lr = analysis_lr.get_best_config(metric="r2", mode="max")
best_ridge = analysis_ridge.get_best_config(metric="r2", mode="max")
best_lasso = analysis_lasso.get_best_config(metric="r2", mode="max")
best_rf = analysis_rf.get_best_config(metric="r2", mode="max")
best_gb = analysis_gb.get_best_config(metric="r2", mode="max")

print("Best hyperparameters for Linear Regression: ", best_lr)
print("Best hyperparameters for Ridge: ", best_ridge)
print("Best hyperparameters for Lasso: ", best_lasso)
print("Best hyperparameters for Random Forest: ", best_rf)
print("Best hyperparameters for Gradient Boosting: ", best_gb)

# Train the best models with the optimal hyperparameters
final_lr = LinearRegression(**best_lr)
final_lr.fit(X_train, y_train)
y_pred_lr = final_lr.predict(X_test)
r2_lr = r2_score(y_test, y_pred_lr)
joblib.dump(final_lr, f"./results/LinearRegression_model.pkl")

final_ridge = Ridge(**best_ridge)
final_ridge.fit(X_train, y_train)
y_pred_ridge = final_ridge.predict(X_test)
r2_ridge = r2_score(y_test, y_pred_ridge)
joblib.dump(final_ridge, f"./results/RidgeRegression_model.pkl")

final_lasso = Lasso(**best_lasso)
final_lasso.fit(X_train, y_train)
y_pred_lasso = final_lasso.predict(X_test)
r2_lasso = r2_score(y_test, y_pred_lasso)
joblib.dump(final_lasso, f"./results/LassoRegression_model.pkl")

final_rf = RandomForestRegressor(**best_rf, random_state=42)
final_rf.fit(X_train, y_train)
y_pred_rf = final_rf.predict(X_test)
r2_rf = r2_score(y_test, y_pred_rf)
joblib.dump(final_rf, f"./results/RandomForest_model.pkl")

final_gb = GradientBoostingRegressor(**best_gb, random_state=42)
final_gb.fit(X_train, y_train)
y_pred_gb = final_gb.predict(X_test)
r2_gb = r2_score(y_test, y_pred_gb)
joblib.dump(final_gb, f"./results/GradientBoosting_model.pkl")

print(f'Best Linear Regression R-squared: {r2_lr}')
print(f'Best Ridge R-squared: {r2_ridge}')
print(f'Best Lasso R-squared: {r2_lasso}')
print(f'Best Random Forest R-squared: {r2_rf}')
print(f'Best Gradient Boosting R-squared: {r2_gb}')

# Compare Models
results = {
    'Model': ['Linear Regression', 'Ridge', 'Lasso', 'Random Forest', 'Gradient Boosting'],
    'R-squared': [r2_lr, r2_ridge, r2_lasso, r2_rf, r2_gb]
}

results_df = pd.DataFrame(results)
print(results_df)

# Save the results to a text file
with open(os.path.join("/app/results", f"test_accuracy.txt"), 'w') as file:
    # Write the variables into the file
    file.write(f"{r2_lr} LinearRegression_model.pkl\n")
    file.write(f"{r2_ridge} RidgeRegression_model.pkl\n")
    file.write(f"{r2_lasso} LassoRegression_model.pkl\n")
    file.write(f"{r2_rf} RandomForest_model.pkl\n")
    file.write(f"{r2_gb} GradientBoosting_model.pkl\n")

print("R2 scores have been written to test_accuracy.txt")
print("Time taken: ", datetime.now()-start_time)
